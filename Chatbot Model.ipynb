{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"how can i initiate a return for a product that i purchased if i'm not satisfied\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Read the JSON data from the file\n",
    "with open('train_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Create a list to store tag and patterns\n",
    "tag_patterns_list = []\n",
    "\n",
    "# Extract \"tag\" and \"patterns\" from each JSON object\n",
    "for item in data:\n",
    "    tag = item[\"tag\"]\n",
    "    patterns = item[\"patterns\"]\n",
    "    tag_patterns_list.append({\"patterns\": patterns[0] if patterns else \"\", \"tag\": tag})\n",
    "\n",
    "# Create a DataFrame with \"patterns\" as the first column and \"tag\" as the second column\n",
    "df = pd.DataFrame(tag_patterns_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "df['patterns'][26]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_availability, return_initiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patterns</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is there a warranty for this product?</td>\n",
       "      <td>product_warranty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the return policy if I'm not satisfied...</td>\n",
       "      <td>product_return</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do you provide free shipping?</td>\n",
       "      <td>product_free_shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are there any discount codes available?</td>\n",
       "      <td>product_discount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are payments via Google Wallet accepted?</td>\n",
       "      <td>payment_google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>When can I expect my order to arrive?</td>\n",
       "      <td>delivery_date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>What is the estimated delivery date?</td>\n",
       "      <td>delivery_date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>What are the features of the latest model of {...</td>\n",
       "      <td>last_model_features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Can you provide a list of the best-selling pro...</td>\n",
       "      <td>product_list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>What is the price range for {product_name} pro...</td>\n",
       "      <td>product_price_range</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             patterns                    tag\n",
       "0               Is there a warranty for this product?       product_warranty\n",
       "1   What is the return policy if I'm not satisfied...         product_return\n",
       "2                       Do you provide free shipping?  product_free_shipping\n",
       "3             Are there any discount codes available?       product_discount\n",
       "4            Are payments via Google Wallet accepted?         payment_google\n",
       "..                                                ...                    ...\n",
       "62              When can I expect my order to arrive?          delivery_date\n",
       "63               What is the estimated delivery date?          delivery_date\n",
       "64  What are the features of the latest model of {...    last_model_features\n",
       "65  Can you provide a list of the best-selling pro...           product_list\n",
       "66  What is the price range for {product_name} pro...    product_price_range\n",
       "\n",
       "[67 rows x 2 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store tag and patterns\n",
    "tag_patterns_list = []\n",
    "\n",
    "# Read data from the test_data.txt file\n",
    "with open('test_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()  # Remove leading/trailing whitespace\n",
    "        tag, patterns = line.split(',', 1)  # Split into tag and patterns\n",
    "        tag = tag.strip('\"')  # Remove double quotes around tag\n",
    "        tag_patterns_list.append({\"patterns\": patterns.strip('\"'), \"tag\": tag})  # Swap tag and patterns\n",
    "\n",
    "# Create a DataFrame for the test data with the desired order\n",
    "test_df = pd.DataFrame(tag_patterns_list)\n",
    "\n",
    "# Display the modified DataFrame for the test data\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tag: image_loading_troubleshooting\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import random\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define a function to preprocess text into bigrams\n",
    "def preprocess_text(text):\n",
    "    # Tokenize data\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Lowercase all words\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "    # Generate bigrams\n",
    "    bigrams_list = list(ngrams(tokens, 1))  # '1' n-gram size\n",
    "\n",
    "    # Lemmatize words (optional)\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [' '.join(bigram) for bigram in bigrams_list]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Define a function to extract POS tags from text\n",
    "def extract_pos_tags(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    return [tag for _, tag in pos_tags]\n",
    "\n",
    "# Define a function to preprocess a single text string and convert it to a feature dictionary\n",
    "def preprocess_text_to_features(text, n=1):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    feature_dict = {' '.join(bigram): True for bigram in preprocessed_text}\n",
    "    \n",
    "    # Extract POS tags and add them as features\n",
    "    pos_tags = extract_pos_tags(text)\n",
    "    for tag in pos_tags:\n",
    "        feature_dict[tag] = True\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "# Prepare the data as labeled featuresets from the 'df' DataFrame\n",
    "labeled_featuresets = []\n",
    "for index, row in df.iterrows():\n",
    "    tag = row['tag']\n",
    "    pattern = row['patterns']\n",
    "    feature_dict = preprocess_text_to_features(pattern)\n",
    "    labeled_featuresets.append((feature_dict, tag))\n",
    "\n",
    "# Shuffle the labeled featuresets to ensure randomness\n",
    "random.shuffle(labeled_featuresets)\n",
    "\n",
    "# Convert feature dictionaries to a feature matrix\n",
    "vectorizer = DictVectorizer()\n",
    "X_train = vectorizer.fit_transform([x for x, _ in labeled_featuresets])\n",
    "y_train = [y for _, y in labeled_featuresets]\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators = 100, random_state = 42, min_samples_split = 4 )\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Define a function to get a tag for a user's question using the Random Forest classifier\n",
    "def get_tag(question, classifier, df):\n",
    "    # Preprocess the question with bigrams and convert it to a feature dictionary\n",
    "    feature_dict = preprocess_text_to_features(question)\n",
    "    \n",
    "    # Extract POS tags and add them as features\n",
    "    pos_tags = extract_pos_tags(question)\n",
    "    for tag in pos_tags:\n",
    "        feature_dict[tag] = True\n",
    "    \n",
    "    # Convert the feature dictionary to a feature vector\n",
    "    feature_vector = vectorizer.transform([feature_dict])\n",
    "    \n",
    "    # Use the classifier to predict the tag\n",
    "    predicted_tag = classifier.predict(feature_vector)[0]\n",
    "    \n",
    "    return predicted_tag\n",
    "\n",
    "# Example usage\n",
    "x = \"The images on your page aren't loading properly.\"\n",
    "predicted_tag = get_tag(x, classifier, df)\n",
    "print(\"Predicted Tag:\", predicted_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.55223880597015 %\n"
     ]
    }
   ],
   "source": [
    "# Prepare the test data as labeled featuresets from the 'test_df' DataFrame\n",
    "test_labeled_featuresets = []\n",
    "for index, row in test_df.iterrows():\n",
    "    tag = row['tag']\n",
    "    pattern = row['patterns']\n",
    "    feature_dict = preprocess_text_to_features(pattern)\n",
    "    test_labeled_featuresets.append((feature_dict, tag))\n",
    "\n",
    "# Convert feature dictionaries to a feature matrix for the test data\n",
    "X_test = vectorizer.transform([x for x, _ in test_labeled_featuresets])\n",
    "y_test = [y for _, y in test_labeled_featuresets]\n",
    "\n",
    "# Use the trained classifier to make predictions on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy*100} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search on hypyerparameters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the trees\n",
    "    'min_samples_split': [2,3,4,5,10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create LOOCV object\n",
    "loo_cv = LeaveOneOut()\n",
    "\n",
    "# Perform grid search with LOOCV\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=loo_cv, scoring='accuracy')\n",
    "\n",
    "# Perform the grid search on your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found by the grid search\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Hyperparameters: 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of the next working with Data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What are the available colors of iPhone 11 and Samsung Galaxy S21?\n",
      "Response: The available colors for iPhone 11 are Red, Green, Yellow.\n",
      "The available colors for Samsung Galaxy S21 are Pink, White, Gray.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import sqlite3\n",
    "\n",
    "# Load the spaCy model with NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Simliar To database:\n",
    "# Function to retrieve product information from the database\n",
    "def get_product_info(product_name):\n",
    "    # Your database retrieval code here (as shown in the previous response)\n",
    "    product_names = [\"iPhone 11\", \"Samsung Galaxy S21\", \"Google Pixel 5\"]\n",
    "    available_colors = [\"Red, Green, Yellow\", \"Pink, White, Gray\", \"Black, White, Green\"]\n",
    "\n",
    "    for index in range(len(product_names)):\n",
    "        if product_names[index] == product_name:\n",
    "            return {\"available_colors\":available_colors[index]}\n",
    "    pass \n",
    "\n",
    "# Function to extract product names from user input using spaCy NER\n",
    "def extract_product_name(user_input):\n",
    "    doc = nlp(user_input)\n",
    "    \n",
    "    # Initialize a list to store extracted product names\n",
    "    product_names = []\n",
    "    \n",
    "    # Extract entities recognized as ORGANIZATION (which can represent product names)\n",
    "    for ent in doc.ents:\n",
    "        # another maybe used in another situations : if ent.label_ in [\"PERSON\", \"ORG\", \"LOC\", \"DATE\", \"TIME\", \"MONEY\"]\n",
    "        if ent.label_ == \"ORG\":\n",
    "            product_names.append(ent.text)\n",
    "    return product_names\n",
    "\n",
    "# User input\n",
    "user_input = \"What are the available colors of iPhone 11 and Samsung Galaxy S21?\"\n",
    "\n",
    "predicted_tag = get_tag(user_input, classifier, df)\n",
    "\n",
    "if predicted_tag == \"product_availability\":\n",
    "    # Extract product names from user input\n",
    "    product_names = extract_product_name(user_input)\n",
    "\n",
    "    # Initialize a dictionary to store product information\n",
    "    product_info_dict = {}\n",
    "\n",
    "    # Retrieve product information from the database for each product name\n",
    "    for product_name in product_names:\n",
    "        product_info = get_product_info(product_name)\n",
    "        if product_info:\n",
    "            product_info_dict[product_name] = product_info\n",
    "\n",
    "    # Generate responses based on the retrieved product information\n",
    "    responses = []\n",
    "    for product_name, product_info in product_info_dict.items():\n",
    "        if 'available_colors' in product_info:\n",
    "            response = f\"The available colors for {product_name} are {product_info['available_colors']}.\"\n",
    "        else:\n",
    "            response = f\"Information about colors for {product_name} is not available.\"\n",
    "        responses.append(response)\n",
    "\n",
    "    if responses:\n",
    "        final_response = \"\\n\".join(responses)\n",
    "    else:\n",
    "        final_response = \"Sorry, I couldn't find information about the specified products.\"\n",
    "\n",
    "    # Print the response\n",
    "    print(\"User Input:\", user_input)\n",
    "    print(\"Response:\", final_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
